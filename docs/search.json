[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Kate Kenny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKate Kenny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKate Kenny\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Kate Kenny’s blog"
  },
  {
    "objectID": "posts/blog1/perceptron.html",
    "href": "posts/blog1/perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "CS 0451\nPerceptron Class Source Code: https://github.com/kate-kenny/kate-kenny.github.io/blob/main/posts/blog1/perceptron.py\nTo implement the perceptron algorithm, I used the above source code to create a perceptron class and perform perceptron updates. The specific update is performed using the fit() method of the class. I implimented the update based on the perceptron equation which is as follows:\n\\[\\textbf{w}^{(t+1)} = \\textbf{w}^{t} + \\mathcal{1}(y_i\\langle \\textbf{w}^{(t)},\\textbf{x}_{i} \\rangle > 0)y_i\\textbf{x}_i\\]\nThe fit() method takes two arguments, a matrix \\(X\\) of features and a vector \\(y\\) of labels. The method then generates a random weight vector, reffered to as \\(w\\) in my source code, and then iterates through the possible maximum number of steps to perform the perceptron update. The actual update occurs in the following way. As enumerated in the description of the algoritm, once we have a random set of weights \\(w\\) we generate a random integer \\(i\\). From there, we index the feature matrix \\(X\\) and the label vector \\(y\\) using that integer \\(i\\) and perform the update which is, in plain language, as follows. The weights in \\(w\\) will be updated if the dot product of the current weights and \\(x_i\\) multiplied by the actual label of the point, \\(y_i\\) is less than zero. In other words, the weights will be changed if the given weights do not corrently label the point \\(x_i\\). If that is the case, we add\n\n\n\nFor the first test case, we are going to run the model on a linearly seperable data set and display the line calculated by the perceptron algorithm to seperate the data points. To do this, I will use the source code provided in class to generate data and then create an instance of my own perceptron class. Finally, we will display the data and seperating line, along with a visual representation of the accuracy over time.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import pyplot as plt2\n\n%load_ext autoreload\n%autoreload 2\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed()\n\nn = 100\np_features = 3\n\n#initialize X, y, a martix of features and vector of labels respectively\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nplt.figure(1)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Test Case 1\")\n\nfrom perceptron import Perceptron\n\n#create instance of perceptron class and use the fit() method on X, y initialized above for the first test case\n\np = Perceptron()\np.fit(X,y)\n\n#draw line from weights found using perceptron algorithm\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.figure(2)\nfig2 = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\n\n\n\nNow, I am going to explore attempting to use the perceptron algorithm on a non-linearly seperable data set.\nFirst, using a similar process as above, I will generate a data set of non-linearly seperated data and then create another instance of the perceptron class. Then, I will use the fit() method to attempt to generate weights, \\(w\\), that will produce a line that will try and categorize the data.\nObviously, the algorithm will not converge but I will display the line generated to seperate the data after the full number of max steps has been concluded and also have a visualization of the accuracy throughout this process.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import pyplot as plt2\n\n%load_ext autoreload\n%autoreload 2\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed()\n\nn = 100\np_features = 3\n\n#initialize X, y, a martix of features and vector of labels respectively\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (.5,.5)])\n\nplt.figure(1)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Test Case 1\")\n\nfrom perceptron import Perceptron\n\n#create instance of perceptron class and use the fit() method on X, y initialized above for the first test case\n\np = Perceptron()\np.fit(X,y)\n\n#draw line from weights found using perceptron algorithm\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.figure(2)\nfig2 = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "warmups/warmup2-20.html",
    "href": "warmups/warmup2-20.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from matplotlib import pyplot as plt \nimport math\nimport numpy as np\n\nfig1, ax1 = plt.subplots(1, 1) \nfig2, ax2 = plt.subplots(1,1)\n\nz = np.linspace(0.01,5)\nt = np.linspace(-1,.999)\n\n\nax1.set(xlabel = r\"$z$\", \n       ylabel = r\"$f(z)$\")\n\n\nax2.set(xlabel = r\"$z$\", \n       ylabel = r\"$g(z)$\")\n\nax1.plot(z, -1*np.log(z))\nax2.plot(t, -1*np.log(1-t))"
  },
  {
    "objectID": "posts/blog1/perceptron.html#conclusion-and-question",
    "href": "posts/blog1/perceptron.html#conclusion-and-question",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Conclusion and Question",
    "text": "Conclusion and Question\nFinally, I want to consider the question posed on the assignment page. What is the runtime complexity of a single iteration of the perceptron algorithm update as described by the update equation?\nRecall the update equation.\n\\[\\textbf{w}^{(t+1)} = \\textbf{w}^{t} + \\mathcal{1}(y_i\\langle \\textbf{w}^{(t)},\\textbf{x}_{i} \\rangle > 0)y_i\\textbf{x}_i\\]\nThe runtime of the equation is \\(O(1)*p\\) where \\(n\\) is the number of data points and \\(p\\) is the number of features. It doesn’t seem like the number of data points \\(n\\) would influence the runtime since a single update is only dealing with one randomly selected data point. So, for the update we need to assign the randomly selected points and corresponding weights. Then, for each update, we also take the dot product of each feature with it’s corresponding weight, which is \\(p\\) operations. Hence, we must multiply the runtime by \\(p\\)."
  },
  {
    "objectID": "warmups/Untitled.html",
    "href": "warmups/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np \n\nalpha = .01\nz_0 = np.random.rand(1, 2)\n\n\ndef f(x): \n    return np.sin(x[0]*x[1])\n\ndef grad(z): \n    return np.array([z[1]*np.cos(z[0]*z[1]), z[0]*np.cos(z[0]*z[1]) \n\ndef find_min(\nwhile diff != 0: \n    z_new = z_0 + aplha*grad(z_0)\n    diff = z_0 - z_new\n    \n    z_0 = z_new\n\nSyntaxError: invalid syntax (1208176260.py, line 14)"
  },
  {
    "objectID": "posts/blog2/Logistic Regression.html",
    "href": "posts/blog2/Logistic Regression.html",
    "title": "Optimization for Linear Regression",
    "section": "",
    "text": "In this blog, I will explore optimization for linear regression through the implementation of three optimization algorithms: simple gradient descent, a momentum method, and stochastic gradient descent. Through experimentation we will compare the performance of each of these algoritms for training linear regression.\n\n\n\nFirst, we will implement standard gradient descent for logistic regression. This algorithm uses the following equation.\nEQUATION\nMy approach to implementing this algorithm was somewhat similar to the way in which I implemented the perceptron algorithm last week. Similarly, the main method in my LogisticRegression class is a fit() method which takes in a matrix of features \\(X\\) and a vector of labels \\(y\\). The fit() method generates a random vector of weights, \\(w\\), and then updates that weight vector based on the gradient descent equation and update step. This results in an algorithm which can estimate the minimum of the loss function and as such gives a hyperplane (a line in our experiments) that can estimate the labels for data that is not linearly seperable.\n\nfrom logistic_regression import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n\n%load_ext autoreload\n%autoreload 2\n\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#create instance of LogisticRegression Class and fit data \nLR = LogisticRegression()\nLR.fit(X, y, alpha=.001)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n#plot line using calculated weights \nfig = draw_line(LR.w, -2, 2)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\nNext, I am going to implement the stochastic gradient descent algorithm for logistic regression optimization. Similar to the standard gradient descent algorithm used above, the stocashtic algorithm can be used to sort data that is not linearly seperable.\nInstead of computing the gradient as we did earlier, for this algorithm we compute the stochastic gradient by picking a random subset \\(S \\in [n]\\) and computing the following.\nEQUATION\nThe implementation of this algorithm is similar to that for normal gradient descent but uses random batches of the data points and iterates through those batches to estimate the minimum of the loss function we are working with.\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n#plot line using calculated weights \nfig = draw_line(LR.w, -2, 2)\n\n\n\n\n\n\n\n\nBelow is an illustration of the loss over epochs for the two algorithms implemented above: gradient descent and stochastic gradient descent.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog() \n\nlegend = plt.legend() \n\n\n\n\n\n\n\nNow, let’s examine a case where gradient descent does not converge because the selected alpha size is too large. Below is a plot of the loss of gradient descent with two different alpha values, .01 and .5.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .5, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = .5\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .01, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = .01\")\n\nplt.loglog() \n\nlegend = plt.legend() \n\n\n\n\nOur next experiment is the case where the batch size influences how quickly our algorithm (stochastic gradient descent) converges. To illustrate this, we are going to run the algorithm on two different batch sizes and examine the loss convergence.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 10)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 100, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 100)\")\n\nplt.loglog() \n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/blog2/Untitled.html",
    "href": "posts/blog2/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from logistic_regression import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n\n%load_ext autoreload\n%autoreload 2\n\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")"
  },
  {
    "objectID": "posts/blog3/Palmer Penguins.html",
    "href": "posts/blog3/Palmer Penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post, we will explore how different models can be used to classify species of penguins in the Palmer Penguin data set and visualize both our results and some of the decisions that went to the models.\n\n\n\nFirst, let’s load our training data into a pandas dataframe and prepare it to be analyzed. Then, we will explore some general trends to see what features could be useful to use in predictions and what kinds of models could be effective to classify the penguin species.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#Prepare Data\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\nBelow is a scatterplot of penguin body mass compared to flipper length. The points in this plot are colored by species, as seen in the legend. This plot highlights that there seems to be some trends in body mass and flipper length by species but the Adelie and Chinstrap penguins are quite similar while the Gentoo penguins have higher body mass and flipper length.\n\nimport seaborn as sns\n\nsns.set_theme()\n\nsns.relplot(data = train, \n           x = \"Body Mass (g)\", \n           y = \"Flipper Length (mm)\", \n           hue = \"Species\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fe6f2df65b0>\n\n\n\n\n\n\n\n\n\nNext, let’s consider the below table which displays the average body mass in grams of the penguins by species and sex. This is an interesting result as the variation by sex of the mass is different for each of the species.\n\ntrain.groupby([\"Species\", \"Sex\"])[[\"Body Mass (g)\"]].mean()\n\n\n\n\n\n  \n    \n      \n      \n      Body Mass (g)\n    \n    \n      Species\n      Sex\n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      3337.280702\n    \n    \n      MALE\n      4020.454545\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      3514.655172\n    \n    \n      MALE\n      3936.111111\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      4875.000000\n    \n    \n      FEMALE\n      4677.976190\n    \n    \n      MALE\n      5502.314815\n    \n  \n\n\n\n\n\n\n\n\n#choose features and do cross validation\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    # \n\n#Model Choices\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n#Plotting decision regions\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef decision_region_panel(X, y, model, qual_features):  \n  p = len(qual_features)\n  fig, axarr = plt.subplots(1, p, figsize=(4*p,4))\n  for i in range(p):\n\n      filler_feature_values = {2+j: 0 for j in range(p)}\n\n      filler_feature_values.update({2+i: 1})\n\n      ix = X[qual_features[i]] == 1\n\n      ax = axarr[i]\n\n      plot_decision_regions(np.array(X[ix]), y[ix], clf=model,\n                            filler_feature_values=filler_feature_values,\n                            filler_feature_ranges={2+j: 0.1 for j in range(p)},\n                            legend=2, ax=ax)\n\n      ax.set_xlabel(X.columns[0])\n      ax.set_ylabel(X.columns[1])\n\n      handles, labels = ax.get_legend_handles_labels()\n      ax.legend(handles, \n          [\"Adelie\", \"Chinstrap\", \"Gentoo\"], \n           framealpha=0.3, scatterpoints=1)\n\n  # Adding axes annotations\n  fig.suptitle(f'Accuracy = {model.score(X, y).round(3)}')\n  plt.tight_layout()\n  plt.show()\n\nqual_features = [\"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ndecision_region_panel(X_train[cols], y_train, LR, qual_features)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\n/Users/katekenny/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n/Users/katekenny/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn("
  },
  {
    "objectID": "posts/blog1/perceptron.html#implementing-the-perceptron-algorithm",
    "href": "posts/blog1/perceptron.html#implementing-the-perceptron-algorithm",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Implementing the Perceptron Algorithm",
    "text": "Implementing the Perceptron Algorithm\n\nKate Kenny\nCS 0451\nPerceptron Class Source Code: https://github.com/kate-kenny/kate-kenny.github.io/blob/main/posts/blog1/perceptron.py\nTo implement the perceptron algorithm, I used the above source code to create a perceptron class and perform perceptron updates. The specific update is performed using the fit() method of the class. I implimented the update based on the perceptron equation which is as follows:\n\\[\\textbf{w}^{(t+1)} = \\textbf{w}^{t} + \\mathcal{1}(y_i\\langle \\textbf{w}^{(t)},\\textbf{x}_{i} \\rangle > 0)y_i\\textbf{x}_i\\]\nThe fit() method takes two arguments, a matrix \\(X\\) of features and a vector \\(y\\) of labels. The method then generates a random weight vector, reffered to as \\(w\\) in my source code, and then iterates through the possible maximum number of steps to perform the perceptron update. The actual update occurs in the following way. As enumerated in the description of the algoritm, once we have a random set of weights \\(w\\) we generate a random integer \\(i\\). From there, we index the feature matrix \\(X\\) and the label vector \\(y\\) using that integer \\(i\\) and perform the update which is, in plain language, as follows. The weights in \\(w\\) will be updated if the dot product of the current weights and \\(x_i\\) multiplied by the actual label of the point, \\(y_i\\) is less than zero. In other words, the weights will be changed if the given weights do not corrently label the point \\(x_i\\). If that is the case, we add\n\n\nExperiment 1\nFor the first test case, we are going to run the model on a linearly seperable data set and display the line calculated by the perceptron algorithm to seperate the data points. To do this, I will use the source code provided in class to generate data and then create an instance of my own perceptron class. Finally, we will display the data and seperating line, along with a visual representation of the accuracy over time.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import pyplot as plt2\n\n%load_ext autoreload\n%autoreload 2\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed()\n\nn = 100\np_features = 3\n\n#initialize X, y, a martix of features and vector of labels respectively\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nplt.figure(1)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Test Case 1\")\n\nfrom perceptron import Perceptron\n\n#create instance of perceptron class and use the fit() method on X, y initialized above for the first test case\n\np = Perceptron()\np.fit(X,y)\n\n#draw line from weights found using perceptron algorithm\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.figure(2)\nfig2 = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\n\n\nExperiment 2\nNow, I am going to explore attempting to use the perceptron algorithm on a non-linearly seperable data set.\nFirst, using a similar process as above, I will generate a data set of non-linearly seperated data and then create another instance of the perceptron class. Then, I will use the fit() method to attempt to generate weights, \\(w\\), that will produce a line that will try and categorize the data.\nObviously, the algorithm will not converge but I will display the line generated to seperate the data after the full number of max steps has been concluded and also have a visualization of the accuracy throughout this process.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import pyplot as plt2\n\n%load_ext autoreload\n%autoreload 2\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed()\n\nn = 100\np_features = 3\n\n#initialize X, y, a martix of features and vector of labels respectively\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (.5,.5)])\n\nplt.figure(1)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Test Case 1\")\n\nfrom perceptron import Perceptron\n\n#create instance of perceptron class and use the fit() method on X, y initialized above for the first test case\n\np = Perceptron()\np.fit(X,y)\n\n#draw line from weights found using perceptron algorithm\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.figure(2)\nfig2 = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  }
]