[
  {
    "objectID": "posts/blog6/Unsupervised Learning.html",
    "href": "posts/blog6/Unsupervised Learning.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "In this"
  },
  {
    "objectID": "posts/blog1/perceptron.html",
    "href": "posts/blog1/perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "CS 0451\nPerceptron Class Source Code: https://github.com/kate-kenny/kate-kenny.github.io/blob/main/posts/blog1/perceptron.py\nTo implement the perceptron algorithm, I used the above source code to create a perceptron class and perform perceptron updates. The specific update is performed using the fit() method of the class. I implimented the update based on the perceptron equation which is as follows:\n\\[\\textbf{w}^{(t+1)} = \\textbf{w}^{t} + \\mathcal{1}(y_i\\langle \\textbf{w}^{(t)},\\textbf{x}_{i} \\rangle > 0)y_i\\textbf{x}_i\\]\nThe fit() method takes two arguments, a matrix \\(X\\) of features and a vector \\(y\\) of labels. The method then generates a random weight vector, reffered to as \\(w\\) in my source code, and then iterates through the possible maximum number of steps to perform the perceptron update. The actual update occurs in the following way. As enumerated in the description of the algoritm, once we have a random set of weights \\(w\\) we generate a random integer \\(i\\). From there, we index the feature matrix \\(X\\) and the label vector \\(y\\) using that integer \\(i\\) and perform the update which is, in plain language, as follows. The weights in \\(w\\) will be updated if the dot product of the current weights and \\(x_i\\) multiplied by the actual label of the point, \\(y_i\\) is less than zero. In other words, the weights will be changed if the given weights do not corrently label the point \\(x_i\\). If that is the case, we add\n\n\n\nFor the first test case, we are going to run the model on a linearly seperable data set and display the line calculated by the perceptron algorithm to seperate the data points. To do this, I will use the source code provided in class to generate data and then create an instance of my own perceptron class. Finally, we will display the data and seperating line, along with a visual representation of the accuracy over time.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import pyplot as plt2\n\n%load_ext autoreload\n%autoreload 2\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed()\n\nn = 100\np_features = 3\n\n#initialize X, y, a martix of features and vector of labels respectively\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nplt.figure(1)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Test Case 1\")\n\nfrom perceptron import Perceptron\n\n#create instance of perceptron class and use the fit() method on X, y initialized above for the first test case\n\np = Perceptron()\np.fit(X,y)\n\n#draw line from weights found using perceptron algorithm\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.figure(2)\nfig2 = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\n\n\n\nNow, I am going to explore attempting to use the perceptron algorithm on a non-linearly seperable data set.\nFirst, using a similar process as above, I will generate a data set of non-linearly seperated data and then create another instance of the perceptron class. Then, I will use the fit() method to attempt to generate weights, \\(w\\), that will produce a line that will try and categorize the data.\nObviously, the algorithm will not converge but I will display the line generated to seperate the data after the full number of max steps has been concluded and also have a visualization of the accuracy throughout this process.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import pyplot as plt2\n\n%load_ext autoreload\n%autoreload 2\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed()\n\nn = 100\np_features = 3\n\n#initialize X, y, a martix of features and vector of labels respectively\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (.5,.5)])\n\nplt.figure(1)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Test Case 1\")\n\nfrom perceptron import Perceptron\n\n#create instance of perceptron class and use the fit() method on X, y initialized above for the first test case\n\np = Perceptron()\np.fit(X,y)\n\n#draw line from weights found using perceptron algorithm\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.figure(2)\nfig2 = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/blog1/perceptron.html#conclusion-and-question",
    "href": "posts/blog1/perceptron.html#conclusion-and-question",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Conclusion and Question",
    "text": "Conclusion and Question\nFinally, I want to consider the question posed on the assignment page. What is the runtime complexity of a single iteration of the perceptron algorithm update as described by the update equation?\nRecall the update equation.\n\\[\\textbf{w}^{(t+1)} = \\textbf{w}^{t} + \\mathcal{1}(y_i\\langle \\textbf{w}^{(t)},\\textbf{x}_{i} \\rangle > 0)y_i\\textbf{x}_i\\]\nThe runtime of the equation is \\(O(1)*p\\) where \\(n\\) is the number of data points and \\(p\\) is the number of features. It doesn’t seem like the number of data points \\(n\\) would influence the runtime since a single update is only dealing with one randomly selected data point. So, for the update we need to assign the randomly selected points and corresponding weights. Then, for each update, we also take the dot product of each feature with it’s corresponding weight, which is \\(p\\) operations. Hence, we must multiply the runtime by \\(p\\)."
  },
  {
    "objectID": "posts/blog8/Engaging with Dr. Gebru.html",
    "href": "posts/blog8/Engaging with Dr. Gebru.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This week, our machine learning class has the opportunity to hear from Dr. Timnit Gebru, a leader in the field of algorithmic bias. Dr. Gebru is the founder of the Distributed Artificial Intelligence Research Institute (DAIR) and the co-founder of Black in AI. Her work gained national attention after she left Google, where she co-lead the Ethical Artificial Intelligence Team, after the company had issues with a paper she co-authored on the dangers of large language models. She is the recipient of numerous awards and accolades for her work on bias both within technologies and within tech companies. I am very excited and thankful for the opportunity to speak with her in our class setting and to hear her speak to a broader audience at the college on Monday evening.\n\n\n\n\nDr. Gebru states that the motivation for this talk was so much of the harm that has been done to Black Americans both through broad institutional racism and through the specific technologies that have been created without Black voices in the room. She states that this is a difficult topic for her, as a Black woman, to discuss in Computer Vision spaces since they are overwhelmingly white and male. She begins with the point that many people developing computer vision technology have hopes and concerns about the uses of these technologies but do not actually know what they are doing in practice.\nGebru states that the same techology will have different pros and cons to people coming from different backgrounds. Surviellance is one example of this, as people who are used to being in heavily survielled areas might immediately threats of computer vision that would not occus to people unfamiliar to such surviellance. Dr. Gebru gives many examples of technologies and startups that using photos or videos, filter or classify people in ways that perpetuate systemic racism and other inequalities. She gives examples in policing systems, automated hiring/interview technologies, and crime predictors based on facial features.\nA major theme throughout the talk is that the work done by computer scientists is not abstract, but rather about and impacting real people. Additionally, there can be algorithms or technologies that work exactly as intented but still perpetuate systemic inequalities and have the largest impacts on already marginalized groups. Gebru asserts that people are often willing to recognize that datasets need to be more diverse but ignore systemic issues behind these systems. People are eager to make things more diverse without thinking critically about whether things like gender recognition or other facial recognition software are necessary or ethical.\ntldr: Making technology fair goes beyond creating technologies that work equally well on everyone. Every technology and data set involves real people on both sides and can perpetuate systemic biases or violate civil liberties despite how well they work in quantitative measures as technology can only amplify an human intent or issues.\n\n\n\nHow do you think the public panic around things like Chat GPT and other similar recent AI systems has obscured the real dangers you discussed in the talk? Is this cultural moment an opportunity to engage the public on these issues since there is a growing discussion around the dangers of AI or are broader issues being ignored in this discussion?"
  },
  {
    "objectID": "posts/blog7/Unsupervised Learning.html",
    "href": "posts/blog7/Unsupervised Learning.html",
    "title": "Unsupervised Learning and Singular Value Decomposition",
    "section": "",
    "text": "In this blog post, we will be exploring unsupervised learning through two examples. We will be working with Singular Value Decomposition (SVD) to do image compression and with Spectral Community Detection to deal with clusters of data. As a result, this blog post is broken into two sections.\n\n\n\n\nThe SVD of a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{mxn}\\) is as follows.\n\\(\\mathbf{A = UDV}^T\\)\nwhere \\(\\mathbf{D}\\) is a diagonal matrix and the matrices \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal matrices. The entries of \\(\\mathbf{D}\\), \\(\\sigma_i\\), give some measure of how large \\(\\textbf{A}\\) is. We can approximate the matrix \\(\\textbf{A}\\) using a representation that only considers the first \\(k\\) columns of \\(\\textbf{U}\\), \\(k\\) values in \\(\\textbf{D}\\) and the first \\(k\\) rows of \\(\\textbf{V}\\).\nIn this post, we are going to use SVD to construct approximations of a greyscale image using different values of \\(k\\).\n\n\nFirst, let’s choose an RGB image and convert it to greyscale. I am selecting a picture of a Nova Scotia Duck Tolling Retriever.\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://images.fineartamerica.com/images/artworkimages/mediumlarge/3/nova-scotia-duck-tolling-retriever-dog-warren-photographic.jpg\"\nimg = read_image(url)\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\n\n\n\nNow, our image is a very large matrix. So we can implement SVD to approximate the picture!\nWe are going to write a few methods in this post included svd_reconstruct() which will reconstruct an image using a given value \\(k\\) and svd_experiment() which will reconstruct an image for a variety of \\(k\\) values and determine the percentage of the original image’s storage needed for each reconstruction. Additionally, we will need to write some methods to view and compare our images and reconstructions.\n\ndef compare_images(self, A, A_):\n    #plots and labels original/reconstructed images\n\n        fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n        axarr[0].imshow(A, cmap = \"Greys\")\n        axarr[0].axis(\"off\")\n        axarr[0].set(title = \"original image\")\n\n        axarr[1].imshow(A_, cmap = \"Greys\")\n        axarr[1].axis(\"off\")\n        axarr[1].set(title = \"reconstructed image\")\n\n\n\nLet’s implement the svd_reconstruct() function that allows us to specify \\(k\\) and perform SVD reconstruction on an image using that value.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef svd_reconstruct(img, k): \n        #reconstructs img from SVD using k values\n\n        A = img\n       \n        U, sigma, V = np.linalg.svd(A)\n        \n        #construct diagonal matrix D whose entries are entires of sigma\n        D = np.zeros_like(A,dtype=float) # matrix of zeros of same shape as A\n        D[:min(A.shape),:min(A.shape)] = np.diag(sigma)\n        \n        #index first k rows/entries/columns of U, D, and V respectively\n        U_ = U[:,:k]\n        D_ = D[:k, :k]\n        V_ = V[:k, :]\n\n        A_ = U_ @ D_ @ V_\n        return A_\n    \n\nNow that we have our reconstruct function, let’s try it for \\(k=5\\) on the image selected above.\n\ncompare_images(grey_img, svd_reconstruct(grey_img, 5))\n\n\n\n\nThis is not a great approximation, but we can start to see our image taking shape even with a \\(k\\) value as low as 5. So now let’s implement our experimentation function and see how different \\(k\\) values perform when reconstructing our image.\n\n\n\n\ndef svd_experiment(img): \n    fig, axarr = plt.subplots(1, 5, figsize = (10, 3))\n\n    k_arr = np.array([5, 10, 20, 30, 100])\n    count = 0\n    \n    for k in k_arr: \n        storage = round((k*k) / (img.shape[0]*img.shape[1]) * 100, 3)\n        axarr[count].imshow(svd_reconstruct(img, k), cmap = \"Greys\")\n        axarr[count].axis(\"off\")\n        axarr[count].set(title = str(k) + \" components, \\n%storage = \" + str(storage))\n        \n        count+= 1\n    \nsvd_experiment(grey_img)\n\n\n\n\nIt is striking how low the percentage of storage needed to store these images is. Part of the reason for this could be the image we are using in our experiment, which has a shape of (900, 744) and consequently requires a large number of pixels for storage itself. This really shows the utility of our reconstructed images as there is no difference to the naked high with 100 components yet the storage demands are vastly different.\n\n\n\n\nNext, we are going to explore unsupervised learning in the context of Laplacian spectral clustering. Specifically, we will be looking at a graph that represents a social network, specifically a karate club. Below is the network.\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\n\nclubs = nx.get_node_attributes(G, \"club\")\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\nSpectral clustering is a method to define clustering as good when we don’t “cut” too many edges. “Cutting” edges in this instance is labelling two conencted nodes with different labels.\nTo implement this, we want to find a vector \\(z\\) that minimizes the normalized cut objective function \\(f(\\textbf{z}, \\textbf{A})\\) which is defined below. Let \\(\\textbf{A}\\) be the adjacency matrix of a graph \\(G\\).\n\\[\nf(z, A) = \\text{cut}({A}, {z})\\left(\\frac{1}{\\text{vol}_{0}(A, z)} + \\frac{1}{\\text{vol}_{1}(A, z)}\\right)\\]\nwhere \\[\n\\text{vol}_{j}({A}{z}) = \\sum_{i = 1}^n \\sum_{i' = 1}^n 1*[{z_i = j}] a_{ii'}\n\\] In other words, \\(\\text{vol}_{j}({A}{z})\\) is the number of edges that have one node in cluster \\(j\\).\nAlthough we cannot solve for \\(z\\) directly, we can approximate \\(z\\) using an eigenvector of the Laplacian matrix, \\(L = (D)^{-1}(D - A)\\) where \\[\n\\textbf{D} = \\left[\\begin{matrix} \\sum_{i = 1}^n a_{i1} & & & \\\\\n    & \\sum_{i = 1}^n a_{i2} & & \\\\\n    &  & \\ddots & \\\\\n    & & & \\sum_{i = 1}^n a_{in}\n\\end{matrix}\\right]\\;.\n\\] \\(z\\) can be approximated by the eigenvector associated with the second smallest eigenvalue of \\(L\\). In the implementation below, we will find \\(L\\) and the associated eigenvector which can approximate \\(z\\) and predict the clustering of the Karate club.\n\n\ndef spectral_clustering(G): \n    #define adjacency matrix A\n    A = nx.adjacency_matrix(G).toarray()\n    \n    #construct diagonal matrix D whose entries are sum of respective row in A\n    diag = np.sum(A, axis = 1)\n    D = np.diag(diag)\n    \n    #calculate L \n    L = (np.linalg.inv(D))@(D - A)\n    \n    #compute eigenvalues and corresponding eigenvectors\n    eigs, eig_vecs = np.linalg.eig(L)\n    \n    #delete min eigenvalue and corresponding eigenvector\n    eigs2 = np.delete(eigs, np.argmin(eigs, axis= None), 0)\n    eig_vecs2 = np.delete(eig_vecs, np.argmin(eigs), 1)\n    \n    z = eig_vecs2[:, np.argmin(eigs2)]\n    return z\n\n#create labels based on z and plot graph\n\nNow that we can calculate \\(z\\), let’s predict the group seperation in the Karate club and illustrate the preiction.\n\nz_ = spectral_clustering(G) > 0\nplot_graph(G, z=z_)\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"steelblue\" if z_[i] == 1 else \"orange\" for i in G.nodes()],\n        edgecolors = \"black\" \n        ) \n\n\n\n\nThese predicted labels are fairly accurate to the actual divisions in the Karate club. Only node 8 is mislabelled so the unsupervised learning implemented seems to be quite a successful example of using spectral clustering."
  },
  {
    "objectID": "posts/blog2/Logistic Regression.html",
    "href": "posts/blog2/Logistic Regression.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "In this blog, I will explore optimization for logistic regression through the implementation of three optimization algorithms: simple gradient descent, a momentum method, and stochastic gradient descent. Through experimentation we will compare the performance of each of these algoritms for training logistic regression to predict binary classifiers for a data set.\nHere is a link to my source code for the LogisticRegression() module: https://github.com/kate-kenny/kate-kenny.github.io/blob/main/posts/blog2/logistic_regression.py\n\n\n\nFirst, we will implement standard gradient descent for logistic regression. This algorithm uses the following equation to calculate the gradient descent for \\(L(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^{n}l(f_{\\mathbf{w}}(\\mathbf{x_i}),y_i)\\) that we derived in class. The gradient can be calculated as follows.\n\\(\\nabla L(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^{n}{(\\sigma (\\hat{y_i}) - y_i)\\mathbf{x_i}}\\)\nMy approach to implementing this algorithm was somewhat similar to the way in which I implemented the perceptron algorithm last week. Similarly, the main method in my LogisticRegression class is a fit() method which takes in a matrix of features \\(X\\) and a vector of labels \\(y\\). The fit() method generates a random vector of weights, \\(w\\), and then updates that weight vector based on the gradient descent equation and update step. This results in an algorithm which can estimate the minimum of the loss function and as such gives a hyperplane (a line in our experiments) that can estimate the labels for data that is not linearly seperable.\n\nfrom logistic_regression import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n\n%load_ext autoreload\n%autoreload 2\n\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#create instance of LogisticRegression Class and fit data \nLR = LogisticRegression()\nLR.fit(X, y, alpha=.001)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n#plot line using calculated weights \nfig = draw_line(LR.w, -2, 2)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\nNext, I am going to implement the stochastic gradient descent algorithm for logistic regression optimization. Similar to the standard gradient descent algorithm used above, the stocashtic algorithm can be used to sort data that is not linearly seperable.\nInstead of computing the gradient as we did earlier, for this algorithm we compute the stochastic gradient by picking a random subset \\(S \\in [n]\\) and computing the following.\n\\(\\nabla L(\\mathbf{w}) = \\frac{1}{|S|} \\sum_{i=1}^{n}{(\\sigma (\\hat{y_i}) - y_i)\\mathbf{x_i}}\\)\nThe implementation of this algorithm is similar to that for gradient descent above but uses random batches of the data points and iterates through those batches to estimate the minimum of the loss function we are working with.\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n#plot line using calculated weights \nfig = draw_line(LR.w, -2, 2)\n\n\n\n\n\n\n\n\nBelow is an illustration of the loss over epochs for the two algorithms implemented above: gradient descent and stochastic gradient descent. Both have an alpha of .001 and have the maximum number of epochs set to 10,000. It is interesting to observe that while both the stochastic and gradient start with similar loss values, the stochastic gradient loss decreases more rapidly the the standard gradient loss, as pictured below.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000,  \n                  alpha = .001) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .001, max_epochs = 10000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog() \n\nlegend = plt.legend() \n\n\n\n\n\n\n\n\n\n\nNow, let’s examine a case where gradient descent does not converge because the selected alpha size is too large. Below is a plot of the loss of gradient descent with two different alpha values, .01 and 10. For this experiment, we are going to run the models on data with 10 feature dimensions.\n\n#Create data with 10 feature dimensions\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = 10\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .01, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"alpha = .01\")\n\n\nlegend = plt.legend() \n\n\n\n\n\n\n\nOur next experiment is the case where the batch size influences how quickly our algorithm (stochastic gradient descent) converges. To illustrate this, we are going to run the algorithm on two different batch sizes and examine the loss convergence. The larger batch size starts out with higher loss than the smaller batch, but it ends up converging around the same time, so the loss improves at a faster rate.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  batch_size = 10, \n                  alpha = .001) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 10)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  batch_size = 100, \n                  alpha = .001) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 100)\")\n\nplt.loglog() \n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/blog5/Auditing Allocative Bias.html",
    "href": "posts/blog5/Auditing Allocative Bias.html",
    "title": "Auditing Alocative Bias",
    "section": "",
    "text": "In this blog post, we are going to examine algorithmic bias through an audit. Using data from the American Community Survey’s Public Use Microdata Sample (PUMS). I will perform an audit on racial bias in a machine learning model that predicts whether or not an individual in employed.To do this, I will begin by downloading data and training a model to make such predictions. Then, we will examine some of the different measures of fairness like predictive parity and error rate before discussing how the model could be biased and what implications that could have in deployment and beyond. This audit will only consider data from New York State.\nTo begin, let’s download the data and get the problem set up.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\nSTATE = \"NY\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nis_white = acs_data[\"RAC1P\"] == 1\nis_black = acs_data[\"RAC1P\"] == 2\n\nacs_data = acs_data[is_white | is_black]\nacs_date = acs_data.copy()\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(162498, 15)\n(162498,)\n(162498,)\n[1 1 1 ... 1 1 1]\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\n\nLet’s first answer some basic questions about the test data that we are working with. For the sake of this blog, we are going to compare directly Black and white individuals and remove individuals with other listed races from the data frame. This is obviously an incomplete picture of New York’s population but allows us to directly compare and analyze specific racial discrepencies between white individuals and Black individuals. Note that in the group column of the data frame, group 1 represents white individuals and group 2 represents Black individuals. Furthermore, employment status is 1 for an individual that is employed and 0 for an unemployed individual.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\ndf.shape[0]\ny_train.mean()\n\n0.4661840951399252\n\n\nThere are 129,998 individuals in this data set. Of those individuals, the proportion of people with the target label 1 (employed individuals) is .46618.\n\ndf.groupby(\"group\")[\"label\"].mean()\n\ngroup\n1    0.474091\n2    0.420689\nName: label, dtype: float64\n\n\nWithin each group, there is a difference in the proportion of individuals with the target label 1. Among white individuals (group 1), the proportion is .474091 and among Black individials (group 2) the proportion is .420689. This is a difference that is likely the result of many historical factors and centuries of systemic racism in the United States and certainly deserves much more focus than is covered in the scope of this blog post. It is important to note that as the base rates for white and Black individuals differ, it is impossible for our model to achieve callibration and error rate balance. However, for now let’s continue. The following table is the result of breaking the data down by race and sex. Note that in the SEX category, 1.0 refers to male and 2.0 refers to female.\n\nimport seaborn as sns\n\nintersectional = df.groupby([\"group\", \"SEX\"])[\"label\"]\nsns.set_theme()\n\nsns.barplot(data=df, x=\"group\", y=\"label\", hue=\"SEX\")\n\n#CHANGE LABELS \n\n<AxesSubplot: xlabel='group', ylabel='label'>\n\n\n\n\n\nIn this chart, it is interesting that white women and Black women have very similar rates of employment. However, white men have a higher rate of employment compared to white women and the opposite is true for Black men and women.\n\n\n\nThe model we are going to use is the Scitkit-Learn Decision Tree Classifier. Before finalizing our model, we can use cross validation to select the depth for the model in order to balance a high training score and limiting overfitting.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\n#cross validation to choose depth\nfrom sklearn.model_selection import cross_val_score\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1)\n\nfor d in range(2, 10):\n    T = DecisionTreeClassifier(max_depth = d)\n    m = cross_val_score(T, X_train, y_train, cv = 10).mean()\n    ax.scatter(d, m, color = \"black\")\n    # ax.scatter(d, T.score(X_test, y_test), color = \"firebrick\")\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n\n0.8037772249581154\n0.8101165062594978\n0.7835820895522388\n\n\n\n\n\nIt seems like a depth of 4 could be ideal as the score improvement slows down significantly when the depth continues to increase beyon 4. So, to prevent against overfitting it seems that 4 is the best choice. Given that, let’s train our model on the available data before moving into testing.\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = 4))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=4))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=4))])StandardScalerStandardScaler()DecisionTreeClassifierDecisionTreeClassifier(max_depth=4)\n\n\n\n\n\nNow that we have trained a model, let’s use it on test data and do an audit surrounding the accuracy, fairness, and allocative bias in the results.\n\ny_hat = model.predict(X_test)\n\nprint(\"Overall accruacy:\")\nprint((y_hat == y_test).mean())\n\nOverall accruacy:\n0.8151692307692308\nAccuracy for white individuals\n0.8174079284348735\nAccuracy for Black individuals\n0.8021770985974461\n\n\nInitially, our model has an accuracy of .815 overall. Next, let’s consider the general confusion matrix for this test data.\n\nfrom sklearn.metrics import confusion_matrix\n\n#calculate false neg, false pos rates\nconfusion_matrix(y_test, y_hat, normalize = \"true\")\n\narray([[0.79528736, 0.20471264],\n       [0.16192053, 0.83807947]])\n\n\nThe false negative rate of our model is .1619 and the false positive rate of our model is .2047.\nNext, let’s consider some of the mathematical measures of fairness that we have been working with in the context of this model. We can measure the calibration, error rate balance, and statistical parity.\n\n#calculate PPV\nnp.sum(y_test) / np.sum(y_hat)\n\n0.9311216624529814\n\n\nThe overall positive predictive values (PPV) of the model is .9311.\nNow that we have some overall measures, let’s dive deeper into the different measures of accuracy by group.\n\nprint(\"Accuracy for white individuals\")\nprint((y_hat == y_test)[group_test == 1].mean())\nprint(\"Accuracy for Black individuals\")\nprint((y_hat == y_test)[group_test == 2].mean())\n\nAccuracy for white individuals\n0.8174079284348735\nAccuracy for Black individuals\n0.8021770985974461\n\n\nWhen we group by race above, we can see that the accuracy for white individuals (.8174) is slightly higher than the accuracy for Black individuals (.8023). Next let’s consider the positive predictive value for both groups. This is the number of predicted labels of 1 divided by the actual number of target labels 1. In other words, it’s how often a positive prediction from the model is correct.\n\nprint(\"PPV for white individuals\")\nprint(np.sum(y_test[group_test == 1]) / np.sum(y_hat[group_test == 1]))\nprint(\"PPV for Black individuals\")\nprint(np.sum(y_test[group_test == 2]) / np.sum(y_hat[group_test == 2]))\n\ndf = pd.DataFrame(X_test, columns = features_to_use)\n\ndf[\"pred_label\"] = 1* y_hat.tolist()\ndf[\"label\"] = y_test.tolist()\ndf[\"race\"] = group_test.tolist()\n\ndf.groupby([\"race\", \"pred_label\"])[\"label\"].mean().reset_index(name = \"mean\")\n\nPPV for white individuals\n0.9366807535276843\nPPV for Black individuals\n0.8967198581560284\n\n\n\n\n\n\n  \n    \n      \n      race\n      pred_label\n      mean\n    \n  \n  \n    \n      0\n      1\n      False\n      0.151795\n    \n    \n      1\n      1\n      True\n      0.787050\n    \n    \n      2\n      2\n      False\n      0.141214\n    \n    \n      3\n      2\n      True\n      0.738918\n    \n  \n\n\n\n\nThe PPV for white individuals if 4 percentage points higher than that for Black individuals. This means that the model is not perfectly calibrated although the disparity is not that extreme.\n\n#Confusion matrix for Black individuals \nconfusion_matrix(y_test[group_test == 2], y_hat[group_test == 2], normalize = \"true\")\n\narray([[0.78612927, 0.21387073],\n       [0.17597627, 0.82402373]])\n\n\n\n#Confusion matrix for white individuals\nconfusion_matrix(y_test[group_test == 1], y_hat[group_test == 1], normalize = \"true\")\n\narray([[0.79700942, 0.20299058],\n       [0.15974612, 0.84025388]])\n\n\nThe false negative rate for Black individuals is .1759 and the false positive rate is .2138. The false negative rate for white individuals is .1597 and the false positive rate is .2138. So this is not an extreme difference but the error rates are not completely balanced.\nThe model would be calibrated if Black and white individuals who were predicted to be unemployed had the same chance of being unemployed. To calculate this, we want to take all the individuals who were predicted to be unemployed and calculate what percentage actually are by group.\nFinally, let’s consider statistical parity. Statistical parity is achieved if a protected group is treated equally by the model as the entire population being considered. So in this context, that would mean Black individuals being predicted to be unemployed with the same accuracy as the general predictions. As calculated above, the overall accuracy is .815. The accuracy for Black individuals is .802. So again, this is a slight difference but there is a discrepency.\nOverall, our model seems to have slight bias against Black Individuals using all of our measures of fairness but none of these disparities is as extreme as the example we examined in class. However, that does not mean it is insignificant. Discrimination is compounding and all of these differences can lead to a combined effect of Black individuals being discriminated against by our model.\n\n\n\nNow, let’s consider the broader implications of the model we have created, what contexts that it could be used in, and how any unfairness by different standards could impact individuals when our model is being used. An algorithm that predicts unemployment could be useful to a variety of companies or interests. People renting homes, giving loans, or providing other social services could all be interested in predicting employment status. I could also see such a model being used for backround checks of various kinds, espeically when information on individuals could be limited or collecting employment status is not permitted in the circumstance.\nBased on my audit, I think that the impact of my model being deployed by a corportation or the government could perpetuate existing economic inequalities for Black individuals. A model that predicts Black unemployment at a higher rate than white individuals could lead to people already facing systemic oppression being denied opportunities or social services. The model constructwed in this post has slight disc rimination according to each measure of fairness we considered. Although these are all relatively small, they all are unfair towards Black individuals implying that implementing the model on a large scale would have a disparate impact on that population. Small, maybe even statistically insignificant, discrimination in many areas or by many standards adds up and does impact the lives of real people.\nBeyond the actual fairness of the model, there are some moral considerations that I would have to take into account before allowing this model to be implemented in the real world. Let’s consider the example of government agencies providing certain social services, housing, health insurance, or other benefits based on whether the model expects an individual to be able to pay for them based on employment status. For example, there have been welfare laws that require employment and I could see this being used to implement similar programs across different social sectors. I personally do not believe it is the job of the government to deny individuals social services or aid if they are unemployed. It would obviously be unfortunate if an individual was denied a service because the model predicted they were unemployed and they were not, but I think the real failure in that instance is making help conditional on employment and only valuing those who “contribute” to society based on capitalist values rather than valuing and prioritizing a social safety net for all."
  },
  {
    "objectID": "posts/blog4/Linear Regression.html",
    "href": "posts/blog4/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Source code: https://github.com/kate-kenny/kate-kenny.github.io/blob/main/posts/blog4/linear_regression.py\nIn this blog, we will be implementing linear regression in two ways. The first is an exact, analytical implementation of least-sqaures linear regression and the second is a gradient descent implementation.\nThe loss function for this empirical risk minimization problem is defined as follows.\n$L() = || ||^2_2 $\nTo start, we want to take the gradient of \\(L(\\textbf{w})\\) with respect to \\(\\textbf{\\hat{w}}\\).\n$ L() = 2$\nFrom here, we will implement both of our fit methods, test them, and then perform some experiements using the linear regression model.\n\n\n\n\n\n\nThis initial example is p=1 for visualization purposes. Later in this post we will experiment with p values of different sizes.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n#generate data \nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\n\nLet’s use this data to test both our analytical and gradient fit methods. We will show that both of our fit methods result in the same values for the prediction vertor \\(\\mathbf{w}\\). The score on the training data and the validation data is calculated using the coefficient of determination, referred to as \\(c\\).\n$ c = 1 - \\frac{{i=1}^{n}{( - y_i)^2}}{{i=1}^{n}{({y} - y_i)^2} $\n\nfrom linear_regression import LinearRegression\n\n\nLR = LinearRegression()\nLR.fit_analytical(pad(X_train), y_train)\n\nprint(f\"Training score = {LR.score(pad(X_train), y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(pad(X_val), y_val).round(4)}\")\n\nLR2 = LinearRegression()\nLR2.fit_gradient(pad(X_train), y_train, alpha = .01, max_iter = 100)\n\nprint(LR.w)\nprint(LR2.w)\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\nTraining score = 0.604\nValidation score = 0.7412\n[0.94220381 0.72060616]\n[0.94211113 0.72065255]\n\n\n\n\n\n\n\n\nThe first experiment we are going to perform is allowing p_features, the number of features, to increase while the number of training points, n_train, remains the same.\n\nLR = LinearRegression()\n\nn_train = 100\nscore_training = []\nscore_val = []\n\nfor i in range(n_train - 1):\n    n_val = 100\n    p_features = i\n    noise = 0.2\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR.fit_analytical(pad(X_train), y_train)\n    score_training.append(LR.score(pad(X_train), y_train).round(4))\n    score_val.append(LR.score(pad(X_val), y_val).round(4))\n    \nplt.plot(score_training)\nplt.plot(score_val)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nHere we see both scores increasing at first but the score val beginning to fluctuate pretty widely once we get past around 65 features while the training score continues to approach and possibly reach 1. This is because the training model is able to refit directly based on the number of features and as the features increase, as does the liklihood overfitting. It appears that we reach a certain point where increasing the number of features no longer increases the accuracy because the model becomes to overfit to accurately make predictions on the test data. That is why we see the score_val starting to drop, slowly at first and then drastically.\n\n\n\n\nNext, we will implement Lasso Regularization using the SciKit Learn implementation.\n\nfrom sklearn.linear_model import Lasso \nL = Lasso(alpha = .001)\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nL.score(X_val, y_val)\n\n0.7432441465308909\n\n\nLet’s repeat the experiment above that we did for Linear Regression with LASSO.\n\nn_train = 100\nscore_training = []\nscore_val = []\n\nfor i in range(n_train - 1):\n    n_val = 100\n    p_features = i\n    noise = 0.2\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(pad(X_train), y_train)\n    score_training.append(L.score(pad(X_train), y_train).round(4))\n    score_val.append(L.score(pad(X_val), y_val).round(4))\n    \nplt.plot(score_training)\nplt.plot(score_val)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nThere are somewhat similar results in this LASSO experiment as we saw for Linear Regression however the variation of the validation score is much smaller. So overall the validation score seems to be higher with LASSO than linear regression. Also similarly to above, it appears that the reason that the validation score drops while training score continues to increase is that our model is overfitting on the training data. The result of this is an extremely high training accuracy (very close to or at 1) and a dropping testing accuracy. This LASSO experiment seems to prevent some of the extreme overfitting we saw above that led to the validation score fluctuating wildly, but the model is still overfitting as the accuracy clearly decreases past a certain number of iterations."
  },
  {
    "objectID": "posts/blog3/Palmer Penguins.html",
    "href": "posts/blog3/Palmer Penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Model Choices\n\nLogistic Regression\n\n#Trying Different Models and Plotting decision regions\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom mlxtend.plotting import plot_decision_regions\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n    \ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Dream\", \"Island_Biscoe\", \"Island_Torgersen\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\nqual_features = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(LR, X_train[cols], y_train)\n\n/Users/katekenny/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n<bound method ClassifierMixin.score of LogisticRegression()>\n\n\n\n\n\n\n\nDecision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nDT = DecisionTreeClassifier(max_depth = 7)\nDT.fit(X_train[cols], y_train)\n\nplot_regions(DT, X_train[cols], y_train)\n\n\n\n\n\nThis model achhieves a training accuracy score of 1, however from the plots alone it is clear that there is some overfitting occurring to achieve the perfect accuracy score. As a result, even though this model has the “best’ training score, it will likely not be the best model to make predictions on new data.\n\n\nSupport Vector Machine\n\nfrom sklearn.svm import SVC \n\n#NEED to cross validate to find gamma here \n\nsv = SVC(kernel=\"rbf\", gamma = 0.1)\nsv.fit(X_train[cols], y_train)\n\nplot_regions(sv, X_train[cols], y_train)\n\n\n\n\n\n\nTesting the models\nNow that we have trained a variety of models, we can score them on test data that the models have not yet seen.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nprint(\"Logistic Regression Score: \") \nprint(LR.score(X_test[cols], y_test))\nprint(\"Decision Tree Score: \") \nprint(DT.score(X_test[cols], y_test))\nprint(\"Support Vector Machine Score: \")\nprint(sv.score(X_test[cols], y_test))\n\n\nLogistic Regression Score: \n1.0\nDecision Tree Score: \n0.9852941176470589\nSupport Vector Machine Score: \n0.9558823529411765\n\n\n\n\nResults and discussion\nFrom the results above, we can see that each of the three models considered and trained performed well with scores above .95. The model that performed the best on the test data was Logistic Regression, which had a perfect score of 1.0. So it seems that our process to choose features and train the models was successful and could be replicated on diifferent similar data sets to classify penguin species."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Kate Kenny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKate Kenny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKate Kenny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKate Kenny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKate Kenny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKate Kenny\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Kate Kenny’s blog"
  }
]